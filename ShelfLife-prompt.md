## ShelfLife prompt

The following text was written to inform the initial prompt for the project. This project description was fed into an agentic LLM tool to generate the initial code:

Your are a coding tutor and librarian. Build an interface for entering data about a collection of books, allowing a minimal amount of info to be included then expanded on with the help of a language model. It will be called ShelfLife and will be useful to book hoarders.

As well as the primary goal of fetching more expansive information on each volume, the Perplexity language model should summarise the book, create a taxonomic tagging system and generate a graph of connections between books in the collection. The ability to update or delete individual books in the collection should be included, also allowing an update of the metadata generated by the language model. Database format should suit the cataloguing of over 1000 books. 

The primary coding goal should be minimal lines and a python single file calling on a database file and one additional file for security related items such as API keys. Consider how to implement this with maximum efficiency and clear code comments.

The final tool should run on a browser, using Streamlit. It is intended to be run on a Raspberry Pi, following an initial build on a Windows PC using Cursor AI text editor with built in LLM assistance.

Here's a speculative use case that can be used to consider the final goals of the interface:

"Arriving home from a visit to a second hand book shop, the book hoarder brings their latest purchase to the library for cataloguing: Typee by Herman Melville, from the Penguin Illustrated Classics series with their irresistible orange spines. They enter the title and author into the interface, but the ISBN number or publishing year is illegible. ShelfLife uses this to fetch missing details from API calls such as Open Library and Google Books, then uses this to query the Perplexity language model API additionally, requesting the return of metadata that includes a genre, brief synopsis, cover image and a taxonomy of themes. The results are presented to the hoarder for approval before committing it to the library to be represented in future on a separate page of the interface. Later, curious about relationships between this book and others in the library, the hoarder will examine the suggested connections between period, author, genre or themes. Finally, an overall summary of the total catalogue is provided as both a statistical summary and written exposition on quantity and quality"

## Core Concept

The proposed system would serve as an intelligent library cataloguing tool that transforms minimal input into rich, interconnected bibliographic data

This aligns well with both computational and humanistic approaches to knowledge organization.

## Architectural Components

**Data Entry Layer**

- Minimal required fields: Title, Author, Year
- Optional fields: ISBN, Publisher, Physical condition
- Image upload capability for book covers

**Enhancement Layer**

- LLM-powered metadata expansion
- Automated taxonomic classification
- Text summarization
- Thematic analysis
- Citation network mapping

**Visualization Layer**

- Force-directed graph visualization of book relationships
- Taxonomic tree visualization
- Timeline view of collection

## Technical Considerations

**Resource Optimization**

- Implement caching for LLM responses
- Batch process API calls to manage rate limits
- Store enhanced data locally to minimize repeated API calls
- Use lightweight visualization libraries suitable for Raspberry Pi

**Data Architecture**

- SQLite for local storage
- JSON for flexible metadata structure
- Vector embeddings for similarity calculations
- Compressed image storage

## Strategic Possibilities

**Enhanced Features**

- Reading list generation based on thematic connections
- Gap analysis in collection coverage
- Collaborative filtering for book recommendations
- Export capabilities for academic reference

**Integration Potential**

- Open Library API for basic metadata
- Google Books API for supplementary data
- Zotero/Mendeley integration for academic collections
- DOI resolution for academic works

## Implementation Strategy

**Phase 1: Core Infrastructure**

1. Basic data entry interface
2. Local database setup
3. API integration

**Phase 2: Enhancement Layer**

1. LLM processing pipeline
2. Taxonomy generation
3. Summary creation

**Phase 3: Visualization**

1. Graph database integration
2. Network visualization
3. Interactive exploration interface